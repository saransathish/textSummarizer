{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcZpSBmsr6pK",
        "outputId": "02f09da7-8f21-4097-c642-944561e72ccb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Very nice...',\n",
              " 'Thanks For Making Amazing Playlist',\n",
              " 'Hello sir,<br>Actually I am very much scared of mathematical part of ML, DL. Your way of teaching makes me to completely understand the concepts clearly. Thanks a lot . <br><br>Can you explain about the concept of Darknet CNN.<br><br>Also please explain how CNN is used in classifying attacks in intrusion detection.',\n",
              " 'The Best teacher I have ever had &lt;3<br>perfectos..',\n",
              " 'can someone tell me are these videos enough for understanding the DL. for interview.',\n",
              " '<a href=\"https://www.youtube.com/watch?v=SH8D4WJBhms&amp;t=31m21s\">31:21</a> he has applied the wrong min max scaler',\n",
              " 'Hello sir ,<br> We are currently given on a project topic &quot; tomato leaf disease classification&quot; sir can you suggest me how to proceed and the best architecture',\n",
              " 'I needed a quick revision before an interview. Your videos never disappoint, Krish Sir. Thanks for the great work<a href=\"about:invalid#zCSafez\"></a>',\n",
              " 'Thankyou sir‚ù§Ô∏èüôågod bless you more and moreüôåüôå',\n",
              " 'Sir we want separate community session for CNN and RNN only',\n",
              " 'Hi Sir, <br>Can you please suggest how we should proceed further in deep learning after completing this series?<br>Thank you',\n",
              " 'Thank you so much Krish..it&#39;s very useful. I learned a lot from your session.',\n",
              " 'The best explanation ever for cnnüíú',\n",
              " 'Thanks for great teaching.....',\n",
              " 'Thanks Krish for your passion and efforts to teach us very smoothly.',\n",
              " 'you are a real life super hero.',\n",
              " 'Thank you so much krish sir for this wonderful playlist of Deep Learning. I learned a lot from it. I really appreciate your teaching style.',\n",
              " '<a href=\"https://www.youtube.com/watch?v=SH8D4WJBhms&amp;t=31m00s\">31:00</a>',\n",
              " 'Can we use optimization using karas tuner when we create convolution 3D',\n",
              " 'Please finish the playlist by adding more on object detection face detection image segmentatiom open cv yolo etc',\n",
              " 'Really nice sessions and explained in an easy way. Thanks for your efforts to explain in nice way',\n",
              " 'your session helped me a lot‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§‚ù§',\n",
              " 'sir me need help in CNN ...<br><br>how we can apply CNN on intrusion detection  system.. plz make video and concept',\n",
              " 'Sir i am searching for inceptionresnetv2 anf lstm algorithm for my project,i didnt able to get it anywhere. Can u please provide me the algorithm.',\n",
              " 'Thank uüòç',\n",
              " 'Great Content. You are my go-to person if I have any doubts related to data science. The best content on the internet. Happy teacher&#39;s day sir.',\n",
              " 'Here i think you have used sobel filter',\n",
              " 'Great session',\n",
              " 'Hi Krish, I want to use an image dataset in tiff format. what do i need to do to run it on tensorflow, beacuse when i load the images it is not showing',\n",
              " 'Very finely explained image classification heads off to you sir and thank you for providing such videos for freeüôèüôè',\n",
              " 'The best',\n",
              " 'Eagerly waiting for transform learning. You are Gem. Thank you for helping us.',\n",
              " 'Thank you very much sir for this amazing sessions. I am grateful to you. üëç',\n",
              " 'Krish. After the end of program, how to verify the image classification',\n",
              " 'Thank you sir for this amazing session',\n",
              " 'Resource not avilable for day 5 in Ineuron Portal. Please upload asap.',\n",
              " 'On which topic would you conduct live sessions next?',\n",
              " 'Why can&#39;t we use ann for image classification. In what way does CNN make the difference for image classification that ANN can&#39;t do.',\n",
              " 'Thanks Krish for this Deep learning tutorials, When I start to learn Deep Learning , my mind polling back after watching 2 videos . But after watching your deep learning tutorials I am very satisfied by myself. Now I am very much interested to learn Deep learning in Depth .',\n",
              " 'sir, what about RNN??',\n",
              " 'Hi Krish! Can you add the study material for day 5 CNN on ineuron?',\n",
              " 'please upload day  5 session on dashboard with resource file',\n",
              " 'Thank you again Krish.. its great you shared examples.. can you please share more examples and also exercises..',\n",
              " 'Krish, pls upload day 5 session on dashboard',\n",
              " 'Thanks Krish',\n",
              " '@Krish Naik <br>Hello sir,<br>Still the Day5 material is not uploaded in i Neauron community site. Can you please ask your backend team to upload the notes',\n",
              " 'Thank you so much sir.',\n",
              " 'Super explanation Sir, thanks',\n",
              " 'I was a bit scared of Deep Learning because of the high level mathematics and numerous concepts. But because of your live sessions I am not only understanding the mathematics part with ease but also am gaining confidence with Deep Learning. A big thanks to you Krish. I will be forever grateful.',\n",
              " 'Sir, please allow everyone to chat and ask questions here',\n",
              " 'Hi, sir. can you made videos on Bert and Transformers. It will be good for us. Thank you sir .',\n",
              " 'Krish sir please make anather community session for eda because that eda session was not that great and proper',\n",
              " 'Bro ur project,model train on leaf images but test  other (like bike,car,etc) output also came plz soluted tell bro plz plz',\n",
              " 'Thank you to the BEST TEACHER Krish',\n",
              " 'I have problem understanding how to do Semantic segmentation using folder based dataset.<br>All the tutorials out there uses APIs. I don&#39;t understand how to use <a href=\"http://tf.data/\">tf.data</a> pipeline, or any other pipeline for this purpose. Tutorials using MNIST dataset teaches absolutely nothing.<br><br>Can you please make a video on how to train semantic segmentation model using normal datasets like ADEChallenge2016 or CamVid ? (these are just examples, I want to know how to build a pipeline to train a model on images in directories)',\n",
              " 'Hi krish, <br>This is great... ‚ù§Ô∏èWill there be next community session on RNN?']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from googleapiclient.discovery import build\n",
        "\n",
        "# Replace 'YOUR_API_KEY' with the API key you obtained\n",
        "api_key = 'AIzaSyB-SYV_r4VqEO55M97b6Kso013iE25j9Xo'\n",
        "\n",
        "# Create a YouTube API client\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "# Specify the video ID for the video whose comments you want to retrieve\n",
        "video_id = 'SH8D4WJBhms'  # Replace 'YOUR_VIDEO_ID' with the actual video ID\n",
        "\n",
        "# Retrieve comments for the video\n",
        "comments = []\n",
        "nextPageToken = None\n",
        "\n",
        "while True:\n",
        "    results = youtube.commentThreads().list(\n",
        "        part='snippet',\n",
        "        videoId=video_id,\n",
        "        maxResults=100,  # You can adjust the number of comments per page\n",
        "        pageToken=nextPageToken\n",
        "    ).execute()\n",
        "\n",
        "    for item in results['items']:\n",
        "        comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n",
        "        comments.append(comment)\n",
        "\n",
        "    nextPageToken = results.get('nextPageToken')\n",
        "\n",
        "    if nextPageToken is None:\n",
        "        break\n",
        "\n",
        "comments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "_SUg7sk9PBDv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "comments = [\n",
        "    \"Thanks For Making Amazing Playlist\",\n",
        "    \"Hello sir,Actually I am very much scared of mathematical part of ML, DL. Your way of teaching makes me to completely understand the concepts clearly. Thanks a lot .Can you explain about the concept of Darknet CNN.Also please explain how CNN is used in classifying attacks in intrusion detection.\",\n",
        "    \"The Best teacher I have ever had perfectos..\",\n",
        "    \"can someone tell me are these videos enough for understanding the DL. for interview.\",\n",
        "    \"he has applied the wrong min max scaler\",\n",
        "    \"Hello sir , We are currently given on a project topic  tomato leaf disease classification sir can you suggest me how to proceed and the best architecture\",\n",
        "    \"I needed a quick revision before an interview. Your videos never disappoint, Krish Sir. Thanks for the great work\",\n",
        "    \"Thankyou sir god bless you more and more\",\n",
        "    \"Sir we want separate community session for CNN and RNN only\",\n",
        "    \"Hi Sir, Can you please suggest how we should proceed further in deep learning after completing this series? Thank you\"\n",
        "    ]\n",
        "\n",
        "summaries = [\n",
        "    \"Amazing Playlist\",\n",
        "    \"scared of mathematical part of ML, DL. classifying attacks intrusion detection.\",\n",
        "    \" Best teacher perfectos\",\n",
        "    \"videos enough for interview.\",\n",
        "    \"wrong min max scaler\",\n",
        "    \"tomato leaf disease classification best architecture\",\n",
        "    \"revision before interview never disappoint great work\",\n",
        "    \"Thankyou sir\",\n",
        "    \"community session for CNN and RNN\",\n",
        "    \"suggest deep learning after completing\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "F_2AIw91udcj"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 100\n",
        "vocab_size = 10000\n",
        "embedding_dim = 128\n",
        "hidden_units = 100\n",
        "batch_size = 32\n",
        "epochs = 50"
      ],
      "metadata": {
        "id": "Q-4oHElWufyc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(comments)\n",
        "sequences = tokenizer.texts_to_sequences(comments)\n",
        "sequences = pad_sequences(sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")"
      ],
      "metadata": {
        "id": "AxRlEC8nuh9l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
        "summary_tokenizer.fit_on_texts(summaries)\n",
        "summary_sequences = summary_tokenizer.texts_to_sequences(summaries)\n",
        "summary_sequences = pad_sequences(summary_sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")\n"
      ],
      "metadata": {
        "id": "t2hbOtPDuj6P"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_len),\n",
        "    tf.keras.layers.LSTM(hidden_units, return_sequences=True),\n",
        "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(vocab_size, activation=\"softmax\"))\n",
        "])"
      ],
      "metadata": {
        "id": "6DA7Rgflul1g"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\")"
      ],
      "metadata": {
        "id": "57sR-V70un6F"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_train = [tf.keras.utils.to_categorical(seq, num_classes=vocab_size) for seq in summary_sequences]\n",
        "X_train = np.array(sequences)\n",
        "Y_train = np.array(Y_train)\n",
        "X_train , Y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VM12sr5ZupvC",
        "outputId": "0a5aecfc-3279-4b1d-abd3-2515e60669f4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[  8,   4,  30,  31,  32,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [ 17,   2,  33,   9,  34,  35,  36,  37,   6,  38,  39,   6,  40,\n",
              "          18,  19,  41,   6,  42,  43,  10,  20,  44,  45,   3,  46,  47,\n",
              "           8,  11,  48,   7,   5,  21,  49,   3,  50,   6,  51,  12,  52,\n",
              "          22,  21,  13,  12,  53,  54,  14,  55,  56,  14,  57,  58,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  3,  23,  59,   9,  60,  61,  62,  63,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  7,  64,  65,  10,  24,  66,  25,  67,   4,  68,   3,  18,   4,\n",
              "          26,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [ 69,  70,  71,   3,  72,  73,  74,  75,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [ 17,   2,  15,  24,  76,  77,  78,  11,  79,  80,  81,  82,  83,\n",
              "          84,   2,   7,   5,  27,  10,  13,  20,  28,  16,   3,  23,  85,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  9,  86,  11,  87,  88,  89,  90,  26,  19,  25,  91,  92,  93,\n",
              "           2,   8,   4,   3,  94,  95,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [ 96,   2,  97,  98,   5,  29,  16,  29,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [  2,  15,  99, 100, 101, 102,   4,  12,  16, 103, 104,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
              "        [105,   2,   7,   5,  22,  27,  13,  15, 106,  28, 107,  14, 108,\n",
              "         109, 110, 111, 112, 113, 114,   5,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "           0,   0,   0,   0,   0,   0,   0,   0,   0]], dtype=int32),\n",
              " array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 1., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.]],\n",
              " \n",
              "        ...,\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.]],\n",
              " \n",
              "        [[0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         [0., 0., 0., ..., 0., 0., 0.],\n",
              "         ...,\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.],\n",
              "         [1., 0., 0., ..., 0., 0., 0.]]], dtype=float32))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNoJoVZbusQ2",
        "outputId": "01141c19-3868-455d-90e5-a31cd15f2d92"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1/1 [==============================] - 9s 9s/step - loss: 9.2079\n",
            "Epoch 2/50\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 9.1966\n",
            "Epoch 3/50\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 9.1837\n",
            "Epoch 4/50\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 9.1682\n",
            "Epoch 5/50\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 9.1481\n",
            "Epoch 6/50\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 9.1201\n",
            "Epoch 7/50\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 9.0787\n",
            "Epoch 8/50\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 9.0145\n",
            "Epoch 9/50\n",
            "1/1 [==============================] - 0s 46ms/step - loss: 8.9165\n",
            "Epoch 10/50\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 8.7817\n",
            "Epoch 11/50\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 8.6202\n",
            "Epoch 12/50\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 8.4456\n",
            "Epoch 13/50\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 8.2663\n",
            "Epoch 14/50\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 8.0848\n",
            "Epoch 15/50\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 7.9001\n",
            "Epoch 16/50\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 7.7101\n",
            "Epoch 17/50\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 7.5125\n",
            "Epoch 18/50\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 7.3060\n",
            "Epoch 19/50\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 7.0895\n",
            "Epoch 20/50\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 6.8629\n",
            "Epoch 21/50\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 6.6262\n",
            "Epoch 22/50\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 6.3799\n",
            "Epoch 23/50\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 6.1244\n",
            "Epoch 24/50\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 5.8606\n",
            "Epoch 25/50\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 5.5896\n",
            "Epoch 26/50\n",
            "1/1 [==============================] - 0s 37ms/step - loss: 5.3126\n",
            "Epoch 27/50\n",
            "1/1 [==============================] - 0s 43ms/step - loss: 5.0313\n",
            "Epoch 28/50\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 4.7471\n",
            "Epoch 29/50\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 4.4618\n",
            "Epoch 30/50\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 4.1769\n",
            "Epoch 31/50\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 3.8939\n",
            "Epoch 32/50\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 3.6143\n",
            "Epoch 33/50\n",
            "1/1 [==============================] - 0s 45ms/step - loss: 3.3393\n",
            "Epoch 34/50\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 3.0706\n",
            "Epoch 35/50\n",
            "1/1 [==============================] - 0s 39ms/step - loss: 2.8096\n",
            "Epoch 36/50\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 2.5577\n",
            "Epoch 37/50\n",
            "1/1 [==============================] - 0s 42ms/step - loss: 2.3168\n",
            "Epoch 38/50\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 2.0885\n",
            "Epoch 39/50\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 1.8745\n",
            "Epoch 40/50\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 1.6766\n",
            "Epoch 41/50\n",
            "1/1 [==============================] - 0s 38ms/step - loss: 1.4963\n",
            "Epoch 42/50\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 1.3346\n",
            "Epoch 43/50\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 1.1918\n",
            "Epoch 44/50\n",
            "1/1 [==============================] - 0s 44ms/step - loss: 1.0678\n",
            "Epoch 45/50\n",
            "1/1 [==============================] - 0s 40ms/step - loss: 0.9616\n",
            "Epoch 46/50\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.8718\n",
            "Epoch 47/50\n",
            "1/1 [==============================] - 0s 36ms/step - loss: 0.7967\n",
            "Epoch 48/50\n",
            "1/1 [==============================] - 0s 35ms/step - loss: 0.7343\n",
            "Epoch 49/50\n",
            "1/1 [==============================] - 0s 32ms/step - loss: 0.6826\n",
            "Epoch 50/50\n",
            "1/1 [==============================] - 0s 34ms/step - loss: 0.6400\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x78c7a0143850>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "filename = 'lstm model.sav'\n",
        "pickle.dump(model, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "POk4c_IKuuSC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = pickle.load(open('lstm model.sav', 'rb'))"
      ],
      "metadata": {
        "id": "OO0Pp8WJuwTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_comments = comments\n",
        "new_sequences = tokenizer.texts_to_sequences(new_comments)\n",
        "new_sequences = pad_sequences(new_sequences, maxlen=max_len, padding=\"post\", truncating=\"post\")"
      ],
      "metadata": {
        "id": "bRAwKBO5uyHu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_summaries = model.predict(new_sequences)\n",
        "\n",
        "predicted_texts = [summary_tokenizer.sequences_to_texts(seq) for seq in summaries]\n",
        "\n",
        "for i, comment in enumerate(new_comments):\n",
        "    print(f\"Comment: {comment}\")\n",
        "    print(f\"Predicted Summary: {predicted_texts[i]}\\n\")\n"
      ],
      "metadata": {
        "id": "D_A96Jr4u0YH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}